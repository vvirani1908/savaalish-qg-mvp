# Savaalâ€‘ish Conceptâ€‘Driven Question Generation (QG) MVP

Generate **conceptual multipleâ€‘choice questions** from long PDFs/text using a **conceptâ€‘driven RAG** pipeline:
extract ideas â†’ retrieve evidence â†’ generate MCQs â†’ score quality â†’ tag difficulty â†’ **export a single JSON**.

> Built for a takeâ€‘home assignment; polished for a public showcase.

## ğŸ§­ Why this repo (for reviewers/recruiters)
- Handles **long documents** with tokenâ€‘aware chunking + retrieval.
- Focuses on **conceptual understanding**, not trivia.
- Clean, modular code with clear **design tradeâ€‘offs** documented in the notebook.
- Single JSON output validated by an included **schema**.

## ğŸš€ Quickstart
```bash
# 0) (Optional) create a venv
python -m venv .venv && source .venv/bin/activate

# 1) Install deps
pip install -r requirements.txt

# 2) Configure secrets (copy and edit)
cp .env.example .env

# 3) Add docs
# Put your PDFs or .txt files into ./docs/

# 4) Run the notebook
# Open Savaalish_QG_MVP.ipynb in Jupyter/VS Code/Colab and run all cells

# Final output:
#   ./output/questions.json
```

## âš™ï¸ Config
Set these in `.env`:
```
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-4o-mini
OPENAI_EMBEDDING=text-embedding-3-large

MAX_QUESTIONS=20
K_CONTEXT=3
QUALITY_THRESHOLD=0.70
```

## ğŸ§© Repository layout
```
.
â”œâ”€â”€ Savaalish_QG_MVP.ipynb      # Main notebook (end-to-end pipeline)
â”œâ”€â”€ qg_output_schema.json       # JSON schema for output validation
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ docs/                       # (your PDFs/txt go here; .gitkept)
â”‚   â””â”€â”€ .gitkeep
â””â”€â”€ output/                     # Generated output (JSON); .gitkept
    â””â”€â”€ .gitkeep
```

## ğŸ—ï¸ Architecture (30â€‘sec view)
```
docs/ (PDF, txt)
   |
   |-- parse & chunk (token-aware windows)
   |         |
   |         `-- map -> combine -> reduce: extract conceptual ideas (LLM)
   |                      |
   |-- embed chunks ------+--> FAISS index
   |                      |
   `-- per-idea retrieve top-k context
                          |
                      question generation (LLM)
                          |
               quality scoring & difficulty (LLM)
                          |
                   output/questions.json
```

## ğŸ§ª Output
- File: `output/questions.json`
- Conforms to: `qg_output_schema.json`
- Each item has: `id, question, choices[A-D], correct_label, idea_summary, source_citations, quality, difficulty`

## ğŸ—ºï¸ Roadmap (nice-to-haves)
- Caching embeddings & LLM calls
- Multiâ€‘doc idea clustering
- Humanâ€‘inâ€‘theâ€‘loop editing UI
- Alternative vector DBs & LLM providers
- Cost estimator & token accounting

---

**License:** MIT.  Â© {year} Vansh Virani
