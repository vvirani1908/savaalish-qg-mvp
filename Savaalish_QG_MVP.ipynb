{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8af7b971",
   "metadata": {},
   "source": [
    "\n",
    "# Scalable Question Generation (MVP) - Inspired by Savaal\n",
    "**Author:** (Your Name)  \n",
    "**Run date:** 2025-09-17 00:57\n",
    "\n",
    "This notebook implements a minimal yet scalable pipeline to generate conceptual multiple-choice questions from large PDFs or text files, drawing inspiration from the Savaal paper's concept-driven RAG approach.\n",
    "\n",
    "Deliverables produced by this notebook:\n",
    "- A single JSON file at `output/questions.json` with all generated questions + metadata.\n",
    "- Clear, well-commented cells explaining design choices.\n",
    "- Optional bonus: automatic quality scoring and difficulty tagging (Bloom's levels).\n",
    "\n",
    "Note: You will need API access to your chosen LLM and embedding model (default prompts assume OpenAI). No secrets are stored in the notebook; set environment variables locally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78523f08",
   "metadata": {},
   "source": [
    "\n",
    "## Quickstart (Checklist)\n",
    "1. Install deps (next cell).  \n",
    "2. Set environment variables (API key) in the Config cell.  \n",
    "3. Put your input documents into the `docs/` folder (PDF or .txt).  \n",
    "4. Run all cells up to \"Generate Questions\".  \n",
    "5. Inspect and optionally filter by quality.  \n",
    "6. Find your final JSON at `output/questions.json`.  \n",
    "7. Record a 3-minute demo walking through: dataflow diagram -> short run -> final JSON.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65ad872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 25.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 1) Install dependencies (run once)\n",
    "# If you're in Colab: uncomment the following line. In local Jupyter, it's okay to run as-is.\n",
    "# Note: FAISS wheel name varies by platform; 'faiss-cpu' works for most.\n",
    "%pip install -q pypdf tiktoken faiss-cpu numpy pandas python-dotenv tqdm rapidfuzz google-generativeai\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9f9cf4",
   "metadata": {},
   "source": [
    "\n",
    "## Config\n",
    "Set API keys via environment variables or `.env` file (not included in submission).  \n",
    "Default uses OpenAI for both chat-completions and embeddings; feel free to swap vendors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e6cf1766",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, pathlib, json, re, math, uuid, time, datetime\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# PDF parsing + chunking\n",
    "from pypdf import PdfReader\n",
    "import tiktoken\n",
    "\n",
    "# Retrieval\n",
    "import faiss\n",
    "\n",
    "# Optional bonus\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# --- Google Gemini setup ---\n",
    "import google.generativeai as genai\n",
    "\n",
    "if os.path.exists(\".env\"):\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\", \"\")\n",
    "GEMINI_MODEL = os.getenv(\"GEMINI_MODEL\", \"gemini-1.5-flash\")          # or gemini-1.5-pro\n",
    "GEMINI_EMBEDDING = os.getenv(\"GEMINI_EMBEDDING\", \"text-embedding-004\") # 768-dim\n",
    "\n",
    "if not GOOGLE_API_KEY:\n",
    "    print(\"⚠️ Set GOOGLE_API_KEY in your .env\")\n",
    "genai.configure(api_key=GOOGLE_API_KEY)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b9acb373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOCS_DIR = /Users/vanshvirani/projects/savaalish-qg-mvp/docs\n",
      "OUTPUT_DIR = /Users/vanshvirani/projects/savaalish-qg-mvp/output\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path().resolve()\n",
    "DOCS_DIR = ROOT / \"docs\"\n",
    "OUTPUT_DIR = ROOT / \"output\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"DOCS_DIR =\", DOCS_DIR)\n",
    "print(\"OUTPUT_DIR =\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd48063",
   "metadata": {},
   "source": [
    "\n",
    "## Design at a glance\n",
    "- Parse PDFs -> chunk text with token-aware windows (overlap to preserve coherence).\n",
    "- Map->Combine->Reduce: extract main ideas per chunk via LLM; lightly de-duplicate/merge.\n",
    "- Retrieve top-k supporting passages per idea using FAISS over embeddings.\n",
    "- Generate one MCQ per idea with grounded context -> shuffle choices to remove positional bias.\n",
    "- Quality: LLM rubric + heuristics (groundedness, clarity, distractor plausibility).  \n",
    "- Difficulty: Bloom's tags (Remember/Understand/Apply/Analyze/Evaluate/Create) collapsed to easy/med/hard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b24fd7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Utility: token-aware chunking\n",
    "def chunk_text(text: str, model_name: str = \"gpt-4o-mini\", max_tokens=800, overlap=120) -> List[str]:\n",
    "    # Use tiktoken encoding as an approximation of tokens\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    toks = enc.encode(text)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(toks):\n",
    "        sub = toks[i : i + max_tokens]\n",
    "        chunks.append(enc.decode(sub))\n",
    "        i += max_tokens - overlap\n",
    "    return chunks\n",
    "\n",
    "# PDF/text loader\n",
    "def load_docs(docs_dir: pathlib.Path) -> List[Tuple[str, str]]:\n",
    "    docs = []\n",
    "    for p in docs_dir.glob(\"**/*\"):\n",
    "        if p.suffix.lower() == \".pdf\":\n",
    "            reader = PdfReader(str(p))\n",
    "            pages = [page.extract_text() or \"\" for page in reader.pages]\n",
    "            docs.append((str(p), \"\\n\".join(pages)))\n",
    "        elif p.suffix.lower() == \".txt\":\n",
    "            docs.append((str(p), p.read_text(encoding=\"utf-8\", errors=\"ignore\")))\n",
    "    if not docs:\n",
    "        print(f\"No documents found in {docs_dir}. Add PDFs or .txt files.\")\n",
    "    return docs\n",
    "\n",
    "# Embeddings\n",
    "def embed_texts(texts: List[str], batch: int = 64) -> np.ndarray:\n",
    "    vecs = []\n",
    "    for i in range(0, len(texts), batch):\n",
    "        for t in texts[i:i+batch]:\n",
    "            # task_type is optional; retrieval_document gives stable behavior\n",
    "            r = genai.embed_content(\n",
    "                model=GEMINI_EMBEDDING,\n",
    "                content=t,\n",
    "                task_type=\"retrieval_document\"\n",
    "            )\n",
    "            vecs.append(r[\"embedding\"])\n",
    "    arr = np.array(vecs, dtype=\"float32\")\n",
    "    return arr\n",
    "# Build FAISS index\n",
    "def build_faiss_index(chunks: List[str]) -> Tuple[faiss.IndexFlatIP, np.ndarray]:\n",
    "    embs = embed_texts(chunks)\n",
    "    faiss.normalize_L2(embs)\n",
    "    index = faiss.IndexFlatIP(embs.shape[1])\n",
    "    index.add(embs)\n",
    "    return index, embs\n",
    "\n",
    "# LLM call helper\n",
    "def chat(system: str, user: str, max_tokens=700, temperature=0.0, as_json=False) -> str:\n",
    "    import google.generativeai as genai\n",
    "    model = genai.GenerativeModel(GEMINI_MODEL, system_instruction=system)\n",
    "    gen_cfg = {\"temperature\": float(temperature), \"max_output_tokens\": int(max_tokens)}\n",
    "    if as_json:\n",
    "        # This makes Gemini return clean JSON (no extra prose)\n",
    "        gen_cfg[\"response_mime_type\"] = \"application/json\"\n",
    "    resp = model.generate_content(user, generation_config=gen_cfg)\n",
    "    return (resp.text or \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c0784fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re\n",
    "\n",
    "def parse_json_loose(s: str):\n",
    "    \"\"\"\n",
    "    Accepts messy model output and returns a Python object.\n",
    "    Handles code fences, leading/trailing prose, and single-quoted JSON.\n",
    "    \"\"\"\n",
    "    if isinstance(s, (dict, list)):\n",
    "        return s\n",
    "    txt = (s or \"\").strip()\n",
    "\n",
    "    # strip code fences like ```json ... ```\n",
    "    txt = re.sub(r\"^```(?:json)?\\s*|\\s*```$\", \"\", txt, flags=re.S)\n",
    "\n",
    "    # find the first JSON object/array block\n",
    "    m = re.search(r\"(\\{.*\\}|\\[.*\\])\", txt, flags=re.S)\n",
    "    if m:\n",
    "        block = m.group(1)\n",
    "        try:\n",
    "            return json.loads(block)\n",
    "        except Exception:\n",
    "            # light fallback: single quotes → double quotes (only when safe)\n",
    "            if '\"' not in block and \"'\" in block:\n",
    "                try:\n",
    "                    return json.loads(block.replace(\"'\", '\"'))\n",
    "                except Exception:\n",
    "                    pass\n",
    "    # final attempt: try plain loads anyway\n",
    "    return json.loads(txt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63fae45",
   "metadata": {},
   "source": [
    "\n",
    "## Load & Chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3d7cc69a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 193 0 (offset 0)\n",
      "Ignoring wrong pointing object 274 0 (offset 0)\n",
      "Ignoring wrong pointing object 293 0 (offset 0)\n",
      "Ignoring wrong pointing object 331 0 (offset 0)\n",
      "Ignoring wrong pointing object 353 0 (offset 0)\n",
      "Ignoring wrong pointing object 373 0 (offset 0)\n",
      "Ignoring wrong pointing object 394 0 (offset 0)\n",
      "Ignoring wrong pointing object 396 0 (offset 0)\n",
      "Ignoring wrong pointing object 404 0 (offset 0)\n",
      "Ignoring wrong pointing object 418 0 (offset 0)\n",
      "Ignoring wrong pointing object 423 0 (offset 0)\n",
      "Ignoring wrong pointing object 436 0 (offset 0)\n",
      "Ignoring wrong pointing object 443 0 (offset 0)\n",
      "Ignoring wrong pointing object 531 0 (offset 0)\n",
      "Ignoring wrong pointing object 541 0 (offset 0)\n",
      "Ignoring wrong pointing object 545 0 (offset 0)\n",
      "Ignoring wrong pointing object 561 0 (offset 0)\n",
      "Ignoring wrong pointing object 599 0 (offset 0)\n",
      "Ignoring wrong pointing object 601 0 (offset 0)\n",
      "Ignoring wrong pointing object 603 0 (offset 0)\n",
      "Ignoring wrong pointing object 616 0 (offset 0)\n",
      "Ignoring wrong pointing object 618 0 (offset 0)\n",
      "Ignoring wrong pointing object 628 0 (offset 0)\n",
      "Ignoring wrong pointing object 646 0 (offset 0)\n",
      "Ignoring wrong pointing object 666 0 (offset 0)\n",
      "Ignoring wrong pointing object 681 0 (offset 0)\n",
      "Ignoring wrong pointing object 683 0 (offset 0)\n",
      "Ignoring wrong pointing object 685 0 (offset 0)\n",
      "Ignoring wrong pointing object 690 0 (offset 0)\n",
      "Ignoring wrong pointing object 692 0 (offset 0)\n",
      "Ignoring wrong pointing object 694 0 (offset 0)\n",
      "Ignoring wrong pointing object 696 0 (offset 0)\n",
      "Ignoring wrong pointing object 703 0 (offset 0)\n",
      "Ignoring wrong pointing object 705 0 (offset 0)\n",
      "Ignoring wrong pointing object 710 0 (offset 0)\n",
      "Ignoring wrong pointing object 713 0 (offset 0)\n",
      "Ignoring wrong pointing object 716 0 (offset 0)\n",
      "Ignoring wrong pointing object 725 0 (offset 0)\n",
      "Ignoring wrong pointing object 728 0 (offset 0)\n",
      "Ignoring wrong pointing object 730 0 (offset 0)\n",
      "Ignoring wrong pointing object 732 0 (offset 0)\n",
      "Ignoring wrong pointing object 737 0 (offset 0)\n",
      "Ignoring wrong pointing object 742 0 (offset 0)\n",
      "Ignoring wrong pointing object 751 0 (offset 0)\n",
      "Ignoring wrong pointing object 753 0 (offset 0)\n",
      "Ignoring wrong pointing object 755 0 (offset 0)\n",
      "Ignoring wrong pointing object 757 0 (offset 0)\n",
      "Ignoring wrong pointing object 759 0 (offset 0)\n",
      "Ignoring wrong pointing object 761 0 (offset 0)\n",
      "Ignoring wrong pointing object 766 0 (offset 0)\n",
      "Ignoring wrong pointing object 768 0 (offset 0)\n",
      "Ignoring wrong pointing object 770 0 (offset 0)\n",
      "Ignoring wrong pointing object 772 0 (offset 0)\n",
      "Ignoring wrong pointing object 774 0 (offset 0)\n",
      "Ignoring wrong pointing object 776 0 (offset 0)\n",
      "Ignoring wrong pointing object 778 0 (offset 0)\n",
      "Ignoring wrong pointing object 780 0 (offset 0)\n",
      "Ignoring wrong pointing object 788 0 (offset 0)\n",
      "Ignoring wrong pointing object 790 0 (offset 0)\n",
      "Ignoring wrong pointing object 792 0 (offset 0)\n",
      "Ignoring wrong pointing object 794 0 (offset 0)\n",
      "Ignoring wrong pointing object 796 0 (offset 0)\n",
      "Ignoring wrong pointing object 798 0 (offset 0)\n",
      "Ignoring wrong pointing object 800 0 (offset 0)\n",
      "Ignoring wrong pointing object 802 0 (offset 0)\n",
      "Ignoring wrong pointing object 804 0 (offset 0)\n",
      "Ignoring wrong pointing object 812 0 (offset 0)\n",
      "Ignoring wrong pointing object 814 0 (offset 0)\n",
      "Ignoring wrong pointing object 816 0 (offset 0)\n",
      "Ignoring wrong pointing object 826 0 (offset 0)\n",
      "Ignoring wrong pointing object 828 0 (offset 0)\n",
      "Ignoring wrong pointing object 830 0 (offset 0)\n",
      "Ignoring wrong pointing object 837 0 (offset 0)\n",
      "Ignoring wrong pointing object 839 0 (offset 0)\n",
      "Ignoring wrong pointing object 844 0 (offset 0)\n",
      "Ignoring wrong pointing object 846 0 (offset 0)\n",
      "Ignoring wrong pointing object 848 0 (offset 0)\n",
      "Ignoring wrong pointing object 850 0 (offset 0)\n",
      "Ignoring wrong pointing object 914 0 (offset 0)\n",
      "Ignoring wrong pointing object 919 0 (offset 0)\n",
      "Ignoring wrong pointing object 945 0 (offset 0)\n",
      "Ignoring wrong pointing object 959 0 (offset 0)\n",
      "Ignoring wrong pointing object 977 0 (offset 0)\n",
      "Ignoring wrong pointing object 1006 0 (offset 0)\n",
      "Ignoring wrong pointing object 1027 0 (offset 0)\n",
      "Ignoring wrong pointing object 1035 0 (offset 0)\n",
      "Ignoring wrong pointing object 1046 0 (offset 0)\n",
      "Ignoring wrong pointing object 1049 0 (offset 0)\n",
      "Ignoring wrong pointing object 1058 0 (offset 0)\n",
      "Ignoring wrong pointing object 1084 0 (offset 0)\n",
      "Ignoring wrong pointing object 1086 0 (offset 0)\n",
      "Ignoring wrong pointing object 1088 0 (offset 0)\n",
      "Ignoring wrong pointing object 1090 0 (offset 0)\n",
      "Ignoring wrong pointing object 1092 0 (offset 0)\n",
      "Ignoring wrong pointing object 1094 0 (offset 0)\n",
      "Ignoring wrong pointing object 1096 0 (offset 0)\n",
      "Ignoring wrong pointing object 1102 0 (offset 0)\n",
      "Ignoring wrong pointing object 1104 0 (offset 0)\n",
      "Ignoring wrong pointing object 1106 0 (offset 0)\n",
      "Ignoring wrong pointing object 1108 0 (offset 0)\n",
      "Ignoring wrong pointing object 1110 0 (offset 0)\n",
      "Ignoring wrong pointing object 1112 0 (offset 0)\n",
      "Ignoring wrong pointing object 1119 0 (offset 0)\n",
      "Ignoring wrong pointing object 1121 0 (offset 0)\n",
      "Ignoring wrong pointing object 1161 0 (offset 0)\n",
      "Ignoring wrong pointing object 1163 0 (offset 0)\n",
      "Ignoring wrong pointing object 1165 0 (offset 0)\n",
      "Ignoring wrong pointing object 1167 0 (offset 0)\n",
      "Ignoring wrong pointing object 1169 0 (offset 0)\n",
      "Ignoring wrong pointing object 1182 0 (offset 0)\n",
      "Ignoring wrong pointing object 1184 0 (offset 0)\n",
      "Ignoring wrong pointing object 1186 0 (offset 0)\n",
      "Ignoring wrong pointing object 1188 0 (offset 0)\n",
      "Ignoring wrong pointing object 1190 0 (offset 0)\n",
      "Ignoring wrong pointing object 1192 0 (offset 0)\n",
      "Ignoring wrong pointing object 1194 0 (offset 0)\n",
      "Ignoring wrong pointing object 1200 0 (offset 0)\n",
      "Ignoring wrong pointing object 1202 0 (offset 0)\n",
      "Ignoring wrong pointing object 1204 0 (offset 0)\n",
      "Ignoring wrong pointing object 1206 0 (offset 0)\n",
      "Ignoring wrong pointing object 1208 0 (offset 0)\n",
      "Ignoring wrong pointing object 1210 0 (offset 0)\n",
      "Ignoring wrong pointing object 1212 0 (offset 0)\n",
      "Ignoring wrong pointing object 1215 0 (offset 0)\n",
      "Ignoring wrong pointing object 1217 0 (offset 0)\n",
      "Ignoring wrong pointing object 1219 0 (offset 0)\n",
      "Ignoring wrong pointing object 1221 0 (offset 0)\n",
      "Ignoring wrong pointing object 1223 0 (offset 0)\n",
      "Ignoring wrong pointing object 1225 0 (offset 0)\n",
      "Ignoring wrong pointing object 1227 0 (offset 0)\n",
      "Ignoring wrong pointing object 1229 0 (offset 0)\n",
      "Ignoring wrong pointing object 1231 0 (offset 0)\n",
      "Ignoring wrong pointing object 1233 0 (offset 0)\n",
      "Ignoring wrong pointing object 1235 0 (offset 0)\n",
      "Ignoring wrong pointing object 1237 0 (offset 0)\n",
      "Ignoring wrong pointing object 1239 0 (offset 0)\n",
      "Ignoring wrong pointing object 1241 0 (offset 0)\n",
      "Ignoring wrong pointing object 1252 0 (offset 0)\n",
      "Ignoring wrong pointing object 1254 0 (offset 0)\n",
      "Ignoring wrong pointing object 1256 0 (offset 0)\n",
      "Ignoring wrong pointing object 1258 0 (offset 0)\n",
      "Ignoring wrong pointing object 1266 0 (offset 0)\n",
      "Ignoring wrong pointing object 1268 0 (offset 0)\n",
      "Ignoring wrong pointing object 1270 0 (offset 0)\n",
      "Ignoring wrong pointing object 1297 0 (offset 0)\n",
      "Ignoring wrong pointing object 1310 0 (offset 0)\n",
      "Ignoring wrong pointing object 1314 0 (offset 0)\n",
      "Ignoring wrong pointing object 1318 0 (offset 0)\n",
      "Ignoring wrong pointing object 1323 0 (offset 0)\n",
      "Ignoring wrong pointing object 1344 0 (offset 0)\n",
      "Ignoring wrong pointing object 1346 0 (offset 0)\n",
      "Ignoring wrong pointing object 1348 0 (offset 0)\n",
      "Ignoring wrong pointing object 1358 0 (offset 0)\n",
      "Ignoring wrong pointing object 1371 0 (offset 0)\n",
      "Ignoring wrong pointing object 1373 0 (offset 0)\n",
      "Ignoring wrong pointing object 1383 0 (offset 0)\n",
      "Ignoring wrong pointing object 1394 0 (offset 0)\n",
      "Ignoring wrong pointing object 1398 0 (offset 0)\n",
      "Ignoring wrong pointing object 1413 0 (offset 0)\n",
      "Ignoring wrong pointing object 1417 0 (offset 0)\n",
      "Ignoring wrong pointing object 1427 0 (offset 0)\n",
      "Ignoring wrong pointing object 1429 0 (offset 0)\n",
      "Ignoring wrong pointing object 1468 0 (offset 0)\n",
      "Ignoring wrong pointing object 1470 0 (offset 0)\n",
      "Ignoring wrong pointing object 1472 0 (offset 0)\n",
      "Ignoring wrong pointing object 1474 0 (offset 0)\n",
      "Ignoring wrong pointing object 1499 0 (offset 0)\n",
      "Ignoring wrong pointing object 1504 0 (offset 0)\n",
      "Ignoring wrong pointing object 1708 0 (offset 0)\n",
      "Ignoring wrong pointing object 1711 0 (offset 0)\n",
      "Ignoring wrong pointing object 1769 0 (offset 0)\n",
      "Ignoring wrong pointing object 1771 0 (offset 0)\n",
      "Ignoring wrong pointing object 1773 0 (offset 0)\n",
      "Ignoring wrong pointing object 1775 0 (offset 0)\n",
      "Ignoring wrong pointing object 1780 0 (offset 0)\n",
      "Ignoring wrong pointing object 1782 0 (offset 0)\n",
      "Ignoring wrong pointing object 1784 0 (offset 0)\n",
      "Ignoring wrong pointing object 1786 0 (offset 0)\n",
      "Ignoring wrong pointing object 1796 0 (offset 0)\n",
      "Ignoring wrong pointing object 1798 0 (offset 0)\n",
      "Ignoring wrong pointing object 1800 0 (offset 0)\n",
      "Ignoring wrong pointing object 1802 0 (offset 0)\n",
      "Ignoring wrong pointing object 1804 0 (offset 0)\n",
      "Ignoring wrong pointing object 1821 0 (offset 0)\n",
      "Ignoring wrong pointing object 1830 0 (offset 0)\n",
      "Ignoring wrong pointing object 1835 0 (offset 0)\n",
      "Ignoring wrong pointing object 1844 0 (offset 0)\n",
      "Ignoring wrong pointing object 1849 0 (offset 0)\n",
      "Ignoring wrong pointing object 1907 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 text-bearing document(s).\n",
      "Loaded 1 document(s).\n",
      "Total chunks: 147\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pypdf.errors import PdfReadError\n",
    "\n",
    "def load_docs(docs_dir: Path):\n",
    "    docs = []\n",
    "    for p in docs_dir.glob(\"**/*\"):\n",
    "        if p.is_dir():\n",
    "            continue\n",
    "\n",
    "        # PDFs\n",
    "        if p.suffix.lower() == \".pdf\":\n",
    "            try:\n",
    "                size = p.stat().st_size\n",
    "                if size < 1024:\n",
    "                    print(f\"⚠️ Skipping {p.name}: too small/empty ({size} bytes).\")\n",
    "                    continue\n",
    "                reader = PdfReader(str(p))\n",
    "                pages = [page.extract_text() or \"\" for page in reader.pages]\n",
    "                text = \"\\n\".join(pages).strip()\n",
    "                if not text:\n",
    "                    print(f\"⚠️ No extractable text in {p.name} (likely scanned/image-only). Use OCR or a .txt for now.\")\n",
    "                    continue\n",
    "                docs.append((str(p), text))\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Skipping unreadable PDF {p.name}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Plain text\n",
    "        elif p.suffix.lower() == \".txt\":\n",
    "            try:\n",
    "                text = p.read_text(encoding=\"utf-8\", errors=\"ignore\").strip()\n",
    "                if not text:\n",
    "                    print(f\"⚠️ Skipping empty txt: {p.name}\")\n",
    "                    continue\n",
    "                docs.append((str(p), text))\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Skipping txt {p.name}: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"Loaded {len(docs)} text-bearing document(s).\")\n",
    "    return docs\n",
    "\n",
    "docs = load_docs(DOCS_DIR)\n",
    "docs_names = [d[0] for d in docs]\n",
    "print(f\"Loaded {len(docs)} document(s).\")\n",
    "all_chunks, chunk_meta = [], []\n",
    "for path, text in docs:\n",
    "    chunks = chunk_text(text, max_tokens=900, overlap=150)\n",
    "    for idx, ch in enumerate(chunks):\n",
    "        all_chunks.append(ch)\n",
    "        chunk_meta.append({\"doc\": path, \"chunk_id\": f\"{path}#chunk{idx}\"})\n",
    "print(f\"Total chunks: {len(all_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab83847",
   "metadata": {},
   "source": [
    "\n",
    "## Map -> Combine -> Reduce: Extract candidate ideas\n",
    "We prompt the LLM to extract 1-3 conceptual ideas per chunk. Then we lightly merge near-duplicates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "455166f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ideas_raw: 441\n",
      "ideas: 278\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "IDEA_SYSTEM = (\n",
    "    \"Extract conceptual ideas from academic/professional prose. \"\n",
    "    \"Return ONLY a JSON array; each item has keys: title (str), summary (str).\"\n",
    ")\n",
    "IDEA_USER_TMPL = (\n",
    "    \"Extract up to 3 non-trivial conceptual IDEAS (definitions, mechanisms, assumptions, trade-offs).\\n\"\n",
    "    \"TEXT:\\n---\\n{chunk}\\n---\\nReturn ONLY JSON.\"\n",
    ")\n",
    "\n",
    "def _normalize_idea_items(obj):\n",
    "    \"\"\"Coerce Gemini outputs to a list of {title, summary} dicts.\"\"\"\n",
    "    # If raw string, try to parse JSON or pull the first JSON-looking array\n",
    "    if isinstance(obj, str):\n",
    "        try:\n",
    "            obj = json.loads(obj)\n",
    "        except Exception:\n",
    "            m = re.search(r\"\\[.*\\]\", obj, re.S)\n",
    "            obj = json.loads(m.group(0)) if m else []\n",
    "\n",
    "    # If dict, look for common container keys\n",
    "    if isinstance(obj, dict):\n",
    "        obj = (\n",
    "            obj.get(\"ideas\")\n",
    "            or obj.get(\"items\")\n",
    "            or obj.get(\"results\")\n",
    "            or obj.get(\"data\")\n",
    "            or obj.get(\"concepts\")\n",
    "            or []\n",
    "        )\n",
    "\n",
    "    # Ensure list\n",
    "    if not isinstance(obj, list):\n",
    "        obj = [obj]\n",
    "\n",
    "    normalized = []\n",
    "    for it in obj:\n",
    "        if isinstance(it, str):\n",
    "            t = it.strip()\n",
    "            if not t:\n",
    "                continue\n",
    "            normalized.append({\"title\": t[:80], \"summary\": t})\n",
    "        elif isinstance(it, dict):\n",
    "            title = (\n",
    "                it.get(\"title\")\n",
    "                or it.get(\"idea\")\n",
    "                or it.get(\"concept\")\n",
    "                or it.get(\"name\")\n",
    "                or \"\"\n",
    "            ).strip()\n",
    "            summary = (\n",
    "                it.get(\"summary\")\n",
    "                or it.get(\"desc\")\n",
    "                or it.get(\"explanation\")\n",
    "                or \"\"\n",
    "            ).strip()\n",
    "            if not title and summary:\n",
    "                title = summary.split(\".\")[0][:80]\n",
    "            if not summary and title:\n",
    "                summary = title\n",
    "            if title or summary:\n",
    "                normalized.append({\"title\": title, \"summary\": summary})\n",
    "    return normalized\n",
    "\n",
    "def extract_ideas_per_chunk(chunks):\n",
    "    ideas = []\n",
    "    for ch in chunks:\n",
    "        out = chat(\n",
    "            IDEA_SYSTEM,\n",
    "            IDEA_USER_TMPL.format(chunk=ch),\n",
    "            max_tokens=400,\n",
    "            as_json=True,          # ← important for clean JSON\n",
    "        )\n",
    "        ideas.extend(_normalize_idea_items(out))\n",
    "    return ideas\n",
    "\n",
    "def dedup_ideas(ideas, thresh=88):\n",
    "    kept = []\n",
    "    for idea in ideas:\n",
    "        if not any(fuzz.token_set_ratio(idea[\"title\"], k[\"title\"]) >= thresh for k in kept):\n",
    "            kept.append(idea)\n",
    "    for i, it in enumerate(kept):\n",
    "        it[\"id\"] = f\"idea_{i+1:04d}\"\n",
    "    return kept\n",
    "\n",
    "\n",
    "ideas_raw = extract_ideas_per_chunk(all_chunks)\n",
    "print(\"ideas_raw:\", len(ideas_raw))\n",
    "ideas = dedup_ideas(ideas_raw, 88)\n",
    "print(\"ideas:\", len(ideas))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05f1309",
   "metadata": {},
   "source": [
    "\n",
    "## Retrieval: Top-k supporting context per idea\n",
    "We index chunks with FAISS and fetch the top-k most relevant passages to ground each question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "67cb1bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index, _embs = build_faiss_index(all_chunks)\n",
    "\n",
    "def retrieve_context_for_idea(idea_text: str, k=3) -> List[Dict[str,str]]:\n",
    "    qvec = embed_texts([idea_text])\n",
    "    faiss.normalize_L2(qvec)\n",
    "    scores, idxs = index.search(qvec, k)\n",
    "    items = []\n",
    "    for rank, (j, sc) in enumerate(zip(idxs[0], scores[0]), 1):\n",
    "        items.append({\n",
    "            \"rank\": rank,\n",
    "            \"score\": float(sc),\n",
    "            \"chunk\": all_chunks[j],\n",
    "            \"meta\": chunk_meta[j]\n",
    "        })\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d966fa9e",
   "metadata": {},
   "source": [
    "\n",
    "## Question Generation\n",
    "One MCQ per idea with grounded context. We also shuffle choices to avoid positional bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3cf692fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "QG_SYSTEM = \"You write exam-quality multiple-choice questions that test conceptual understanding.\\n- 1 question only, grounded in the given context.\\n- 4 options (A-D), with exactly one correct.\\n- No trivial recall (dates/numbers) unless core to the concept.\\n- Avoid vague or ambiguous wording.\\nReturn strict JSON: {\\\"question\\\": str, \\\"choices\\\": [{\\\"label\\\":\\\"A\\\",\\\"text\\\":...},...], \\\"correct_label\\\":\\\"A\\\"}\"\n",
    "QG_USER_TMPL = \"Idea summary: {idea}\\nUse these supporting snippets (may be partial) to craft 1 conceptual question:\\n{contexts}\\nReturn ONLY the specified JSON.\"\n",
    "\n",
    "import random\n",
    "def shuffle_choices(payload: Dict[str,Any]) -> Dict[str,Any]:\n",
    "    choices = payload[\"choices\"]\n",
    "    # track which is correct BEFORE shuffle\n",
    "    correct = payload[\"correct_label\"]\n",
    "    correct_text = next(c[\"text\"] for c in choices if c[\"label\"]==correct)\n",
    "    # shuffle\n",
    "    labels = [\"A\",\"B\",\"C\",\"D\"]\n",
    "    random.shuffle(choices)\n",
    "    # reassign labels\n",
    "    out_choices = []\n",
    "    new_correct = None\n",
    "    for lab, ch in zip(labels, choices):\n",
    "        out_choices.append({\"label\": lab, \"text\": ch[\"text\"]})\n",
    "        if ch[\"text\"] == correct_text:\n",
    "            new_correct = lab\n",
    "    payload[\"choices\"] = out_choices\n",
    "    payload[\"correct_label\"] = new_correct\n",
    "    return payload\n",
    "\n",
    "def generate_question_for_idea(idea: Dict[str,str], k=3) -> Dict[str,Any]:\n",
    "    ctx_items = retrieve_context_for_idea(idea[\"summary\"], k=k)\n",
    "    ctx_str = \"\\\\n---\\\\n\".join([it[\"chunk\"][:1200] for it in ctx_items])\n",
    "    out = chat(QG_SYSTEM, QG_USER_TMPL.format(idea=idea[\"summary\"], contexts=ctx_str), max_tokens=700, as_json=True)\n",
    "    data = parse_json_loose(out)\n",
    "    data = shuffle_choices(data)\n",
    "    data[\"id\"] = idea[\"id\"]\n",
    "    data[\"idea_summary\"] = idea[\"summary\"]\n",
    "    data[\"source_citations\"] = [f\"{it['meta']['doc']}|{it['meta']['chunk_id']}\" for it in ctx_items]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c822d51",
   "metadata": {},
   "source": [
    "\n",
    "## Quality Control (Bonus)\n",
    "We score each question on: clarity, groundedness, non-triviality, distractor quality -> average to an overall score.  \n",
    "Threshold (default >= 0.7) filters out weaker items.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fdedf5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "QC_SYSTEM = \"You are grading a multiple-choice question with rubric 0.0-1.0.\\nCriteria:\\n- clarity: clear, unambiguous stem\\n- groundedness: answer supported by provided context\\n- non_triviality: requires understanding (not copy-paste recall)\\n- distractor_quality: plausible but clearly incorrect\\nReturn JSON: {\\\"clarity\\\":x,\\\"groundedness\\\":x,\\\"non_triviality\\\":x,\\\"distractor_quality\\\":x,\\\"notes\\\":str}\"\n",
    "QC_USER_TMPL = \"Question:\\n{q}\\nChoices: {choices}\\nCorrect: {correct}\\nContext (evidence):\\n{ctx}\\n\"\n",
    "\n",
    "def score_question(item: Dict[str,Any]) -> Dict[str,Any]:\n",
    "    ctx = \"\\\\n---\\\\n\".join(item[\"source_citations\"])\n",
    "    q = item[\"question\"]\n",
    "    ch = \"; \".join([f\"{c['label']}) {c['text']}\" for c in item[\"choices\"]])\n",
    "    out = chat(QC_SYSTEM, QC_USER_TMPL.format(q=q, choices=ch, correct=item[\"correct_label\"], ctx=ctx), max_tokens=400, as_json=True)\n",
    "    try:\n",
    "        \n",
    "       scores = parse_json_loose(out)\n",
    "\n",
    "\n",
    "    except Exception:\n",
    "        scores = {\"clarity\":0.6,\"groundedness\":0.6,\"non_triviality\":0.6,\"distractor_quality\":0.6,\"notes\":\"parse-fallback\"}\n",
    "    import numpy as np\n",
    "    overall = float(np.mean([scores.get(\"clarity\",0), scores.get(\"groundedness\",0),\n",
    "                             scores.get(\"non_triviality\",0), scores.get(\"distractor_quality\",0)]))\n",
    "    item[\"quality\"] = {\"overall\": round(overall,3), **{k: round(float(scores.get(k,0)),3) for k in [\"clarity\",\"groundedness\",\"non_triviality\",\"distractor_quality\"]}, \"notes\": scores.get(\"notes\",\"\")}\n",
    "    return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c26dab5",
   "metadata": {},
   "source": [
    "\n",
    "## Difficulty Tagging (Bonus)\n",
    "We map Bloom levels -> easy/medium/hard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f201bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DIFF_SYSTEM = \"You are a psychometrics expert. Classify the question's Bloom level (Remember, Understand, Apply, Analyze, Evaluate, Create).\\nReturn JSON: {\\\"bloom\\\": \\\"Understand\\\"}\"\n",
    "\n",
    "def add_difficulty(item: Dict[str,Any]) -> Dict[str,Any]:\n",
    "    text = item[\"question\"] + \" Choices: \" + \"; \".join([c[\"text\"] for c in item[\"choices\"]])\n",
    "    out = chat(DIFF_SYSTEM, text, max_tokens=100)\n",
    "    try:\n",
    "        bloom = json.loads(out).get(\"bloom\",\"Understand\")\n",
    "    except Exception:\n",
    "        bloom = \"Understand\"\n",
    "    mapping = {\"Remember\":\"easy\",\"Understand\":\"easy\",\"Apply\":\"medium\",\"Analyze\":\"medium\",\"Evaluate\":\"hard\",\"Create\":\"hard\"}\n",
    "    item[\"difficulty\"] = mapping.get(bloom, \"medium\")\n",
    "    return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f254929d",
   "metadata": {},
   "source": [
    "\n",
    "## Generate Questions\n",
    "Adjust MAX_QUESTIONS as needed. For long docs, the pipeline amortizes costs and scales better than naive prompting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "05ac1787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Qs:  65%|██████▌   | 13/20 [00:44<00:20,  2.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!! Skipping idea_0013 due to parse error: Expecting ',' delimiter: line 1 column 525 (char 524)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Qs: 100%|██████████| 20/20 [01:05<00:00,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: 19  /  Kept after quality filter (>=0.7): 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAX_QUESTIONS = int(os.environ.get(\"MAX_QUESTIONS\", \"20\"))\n",
    "K_CONTEXT = int(os.environ.get(\"K_CONTEXT\", \"3\"))\n",
    "QUALITY_THRESHOLD = float(os.environ.get(\"QUALITY_THRESHOLD\", \"0.70\"))\n",
    "\n",
    "results = []\n",
    "for idea in tqdm(ideas[:MAX_QUESTIONS], desc=\"Generating Qs\"):\n",
    "    try:\n",
    "        item = generate_question_for_idea(idea, k=K_CONTEXT)\n",
    "        item = score_question(item)\n",
    "        item = add_difficulty(item)\n",
    "        results.append(item)\n",
    "    except Exception as e:\n",
    "        print(f\"!! Skipping {idea.get('id')} due to parse error: {e}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "# Filter by quality\n",
    "filtered = [r for r in results if r.get(\"quality\",{}).get(\"overall\",0) >= QUALITY_THRESHOLD]\n",
    "print(f\"Generated: {len(results)}  /  Kept after quality filter (>={QUALITY_THRESHOLD}): {len(filtered)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41a2b15",
   "metadata": {},
   "source": [
    "\n",
    "## Save JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a27e0749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved -> /Users/vanshvirani/projects/savaalish-qg-mvp/output/questions.json\n"
     ]
    }
   ],
   "source": [
    "out = {\n",
    "    \"run_metadata\": {\n",
    "        \"created_at\": datetime.datetime.now().isoformat(),\n",
    "        \"docs\": docs_names,\n",
    "        \"model\": GEMINI_MODEL,\n",
    "        \"embedding_model\": GEMINI_EMBEDDING,\n",
    "        \"k_context\": K_CONTEXT,\n",
    "        \"cost_estimate_usd\": None\n",
    "    },\n",
    "    \"questions\": filtered\n",
    "}\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "with open(OUTPUT_DIR / \"questions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(out, f, ensure_ascii=False, indent=2)\n",
    "print(\"Saved ->\", OUTPUT_DIR / \"questions.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059ad5ba",
   "metadata": {},
   "source": [
    "\n",
    "## Appendix: 30-second Architecture\n",
    "```\n",
    "docs/ (PDF, txt)\n",
    "   |\n",
    "   |-- parse & chunk (token-aware windows)\n",
    "   |         |\n",
    "   |         `-- map->combine->reduce: extract conceptual ideas (LLM)\n",
    "   |                      |\n",
    "   |-- embed chunks ------+--> FAISS index\n",
    "   |                      |\n",
    "   `-- per-idea retrieve top-k context\n",
    "                          |\n",
    "                      question generation (LLM)\n",
    "                          |\n",
    "               quality scoring & difficulty (LLM)\n",
    "                          |\n",
    "                   output/questions.json\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
