{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8af7b971",
   "metadata": {},
   "source": [
    "\n",
    "# Scalable Question Generation (MVP) - Inspired by Savaal\n",
    "**Author:** (Your Name)  \n",
    "**Run date:** 2025-09-17 00:57\n",
    "\n",
    "This notebook implements a minimal yet scalable pipeline to generate conceptual multiple-choice questions from large PDFs or text files, drawing inspiration from the Savaal paper's concept-driven RAG approach.\n",
    "\n",
    "Deliverables produced by this notebook:\n",
    "- A single JSON file at `output/questions.json` with all generated questions + metadata.\n",
    "- Clear, well-commented cells explaining design choices.\n",
    "- Optional bonus: automatic quality scoring and difficulty tagging (Bloom's levels).\n",
    "\n",
    "Note: You will need API access to your chosen LLM and embedding model (default prompts assume OpenAI). No secrets are stored in the notebook; set environment variables locally.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78523f08",
   "metadata": {},
   "source": [
    "\n",
    "## Quickstart (Checklist)\n",
    "1. Install deps (next cell).  \n",
    "2. Set environment variables (API key) in the Config cell.  \n",
    "3. Put your input documents into the `docs/` folder (PDF or .txt).  \n",
    "4. Run all cells up to \"Generate Questions\".  \n",
    "5. Inspect and optionally filter by quality.  \n",
    "6. Find your final JSON at `output/questions.json`.  \n",
    "7. Record a 3-minute demo walking through: dataflow diagram -> short run -> final JSON.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ad872",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Install dependencies (run once)\n",
    "# If you're in Colab: uncomment the following line. In local Jupyter, it's okay to run as-is.\n",
    "# Note: FAISS wheel name varies by platform; 'faiss-cpu' works for most.\n",
    "%pip install -q pypdf tiktoken faiss-cpu numpy pandas openai python-dotenv tqdm rapidfuzz\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9f9cf4",
   "metadata": {},
   "source": [
    "\n",
    "## Config\n",
    "Set API keys via environment variables or `.env` file (not included in submission).  \n",
    "Default uses OpenAI for both chat-completions and embeddings; feel free to swap vendors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf1766",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, pathlib, json, re, math, uuid, time, datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "# LLM + embeddings (default: OpenAI)\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "# PDF parsing + chunking\n",
    "from pypdf import PdfReader\n",
    "import tiktoken\n",
    "\n",
    "# Retrieval\n",
    "import faiss\n",
    "\n",
    "# Optional bonus\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "# Project paths\n",
    "ROOT = pathlib.Path().resolve()\n",
    "DOCS_DIR = ROOT / \"docs\"\n",
    "OUTPUT_DIR = ROOT / \"output\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Load secrets if present\n",
    "if os.path.exists(\".env\"):\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\", \"\")\n",
    "OPENAI_MODEL = os.environ.get(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n",
    "OPENAI_EMBEDDING = os.environ.get(\"OPENAI_EMBEDDING\", \"text-embedding-3-large\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"Set OPENAI_API_KEY in your environment. Example: export OPENAI_API_KEY=sk-...\")\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd48063",
   "metadata": {},
   "source": [
    "\n",
    "## Design at a glance\n",
    "- Parse PDFs -> chunk text with token-aware windows (overlap to preserve coherence).\n",
    "- Map->Combine->Reduce: extract main ideas per chunk via LLM; lightly de-duplicate/merge.\n",
    "- Retrieve top-k supporting passages per idea using FAISS over embeddings.\n",
    "- Generate one MCQ per idea with grounded context -> shuffle choices to remove positional bias.\n",
    "- Quality: LLM rubric + heuristics (groundedness, clarity, distractor plausibility).  \n",
    "- Difficulty: Bloom's tags (Remember/Understand/Apply/Analyze/Evaluate/Create) collapsed to easy/med/hard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24fd7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Utility: token-aware chunking\n",
    "def chunk_text(text: str, model_name: str = \"gpt-4o-mini\", max_tokens=800, overlap=120) -> List[str]:\n",
    "    # Use tiktoken encoding as an approximation of tokens\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    toks = enc.encode(text)\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(toks):\n",
    "        sub = toks[i : i + max_tokens]\n",
    "        chunks.append(enc.decode(sub))\n",
    "        i += max_tokens - overlap\n",
    "    return chunks\n",
    "\n",
    "# PDF/text loader\n",
    "def load_docs(docs_dir: pathlib.Path) -> List[Tuple[str, str]]:\n",
    "    docs = []\n",
    "    for p in docs_dir.glob(\"**/*\"):\n",
    "        if p.suffix.lower() == \".pdf\":\n",
    "            reader = PdfReader(str(p))\n",
    "            pages = [page.extract_text() or \"\" for page in reader.pages]\n",
    "            docs.append((str(p), \"\\n\".join(pages)))\n",
    "        elif p.suffix.lower() == \".txt\":\n",
    "            docs.append((str(p), p.read_text(encoding=\"utf-8\", errors=\"ignore\")))\n",
    "    if not docs:\n",
    "        print(f\"No documents found in {docs_dir}. Add PDFs or .txt files.\")\n",
    "    return docs\n",
    "\n",
    "# Embeddings\n",
    "def embed_texts(texts: List[str], batch=64) -> np.ndarray:\n",
    "    vecs = []\n",
    "    for i in range(0, len(texts), batch):\n",
    "        resp = client.embeddings.create(model=OPENAI_EMBEDDING, input=texts[i:i+batch])\n",
    "        vecs.extend([d.embedding for d in resp.data])\n",
    "    return np.array(vecs, dtype=\"float32\")\n",
    "\n",
    "# Build FAISS index\n",
    "def build_faiss_index(chunks: List[str]) -> Tuple[faiss.IndexFlatIP, np.ndarray]:\n",
    "    embs = embed_texts(chunks)\n",
    "    faiss.normalize_L2(embs)\n",
    "    index = faiss.IndexFlatIP(embs.shape[1])\n",
    "    index.add(embs)\n",
    "    return index, embs\n",
    "\n",
    "# LLM call helper\n",
    "def chat(system: str, user: str, max_tokens=700, temperature=0.0) -> str:\n",
    "    resp = client.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        messages=[{\"role\":\"system\",\"content\":system},{\"role\":\"user\",\"content\":user}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63fae45",
   "metadata": {},
   "source": [
    "\n",
    "## Load & Chunk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7cc69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docs = load_docs(DOCS_DIR)\n",
    "docs_names = [d[0] for d in docs]\n",
    "print(f\"Loaded {len(docs)} document(s).\")\n",
    "all_chunks, chunk_meta = [], []\n",
    "for path, text in docs:\n",
    "    chunks = chunk_text(text, max_tokens=900, overlap=150)\n",
    "    for idx, ch in enumerate(chunks):\n",
    "        all_chunks.append(ch)\n",
    "        chunk_meta.append({\"doc\": path, \"chunk_id\": f\"{path}#chunk{idx}\"})\n",
    "print(f\"Total chunks: {len(all_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab83847",
   "metadata": {},
   "source": [
    "\n",
    "## Map -> Combine -> Reduce: Extract candidate ideas\n",
    "We prompt the LLM to extract 1-3 conceptual ideas per chunk. Then we lightly merge near-duplicates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455166f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IDEA_SYSTEM = \"You are extracting conceptual ideas from academic/professional prose. Return a compact JSON array; each item MUST have: {\\\"title\\\": str, \\\"summary\\\": str}.\"\n",
    "IDEA_USER_TMPL = \"Extract up to 3 key conceptual ideas (not mere facts) from the following text. Focus on definitions, mechanisms, assumptions, or trade-offs likely to be tested.\\nText:\\n---\\n{chunk}\\n---\\nReturn ONLY a compact JSON array.\"\n",
    "\n",
    "def extract_ideas_per_chunk(chunks: List[str]) -> List[Dict[str,str]]:\n",
    "    ideas = []\n",
    "    for ch in tqdm(chunks, desc=\"Extracting ideas\"):\n",
    "        out = chat(IDEA_SYSTEM, IDEA_USER_TMPL.format(chunk=ch), max_tokens=400)\n",
    "        # Robust parse: try json; fallback simple pattern\n",
    "        try:\n",
    "            arr = json.loads(out)\n",
    "            for it in arr:\n",
    "                title = (it.get(\"title\") or it.get(\"idea\") or \"\").strip()\n",
    "                summary = (it.get(\"summary\") or it.get(\"desc\") or \"\").strip()\n",
    "                if title and summary:\n",
    "                    ideas.append({\"title\": title, \"summary\": summary})\n",
    "        except Exception:\n",
    "            for m in re.findall(r'\"title\"\\\\s*:\\\\s*\"([^\"]+)\"\\\\s*,\\\\s*\"summary\"\\\\s*:\\\\s*\"([^\"]+)\"', out):\n",
    "                ideas.append({\"title\": m[0], \"summary\": m[1]})\n",
    "    return ideas\n",
    "\n",
    "# Lightweight dedup based on title similarity\n",
    "def dedup_ideas(ideas: List[Dict[str,str]], thresh=85) -> List[Dict[str,str]]:\n",
    "    kept = []\n",
    "    for idea in ideas:\n",
    "        if not any(fuzz.token_set_ratio(idea[\"title\"], k[\"title\"]) >= thresh for k in kept):\n",
    "            kept.append(idea)\n",
    "    # assign IDs\n",
    "    for i, it in enumerate(kept):\n",
    "        it[\"id\"] = f\"idea_{i+1:04d}\"\n",
    "    return kept\n",
    "\n",
    "ideas_raw = extract_ideas_per_chunk(all_chunks)\n",
    "ideas = dedup_ideas(ideas_raw, thresh=88)\n",
    "print(f\"Ideas raw: {len(ideas_raw)}  ->  deduped: {len(ideas)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05f1309",
   "metadata": {},
   "source": [
    "\n",
    "## Retrieval: Top-k supporting context per idea\n",
    "We index chunks with FAISS and fetch the top-k most relevant passages to ground each question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cb1bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index, _embs = build_faiss_index(all_chunks)\n",
    "\n",
    "def retrieve_context_for_idea(idea_text: str, k=3) -> List[Dict[str,str]]:\n",
    "    qvec = embed_texts([idea_text])\n",
    "    faiss.normalize_L2(qvec)\n",
    "    scores, idxs = index.search(qvec, k)\n",
    "    items = []\n",
    "    for rank, (j, sc) in enumerate(zip(idxs[0], scores[0]), 1):\n",
    "        items.append({\n",
    "            \"rank\": rank,\n",
    "            \"score\": float(sc),\n",
    "            \"chunk\": all_chunks[j],\n",
    "            \"meta\": chunk_meta[j]\n",
    "        })\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d966fa9e",
   "metadata": {},
   "source": [
    "\n",
    "## Question Generation\n",
    "One MCQ per idea with grounded context. We also shuffle choices to avoid positional bias.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf692fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "QG_SYSTEM = \"You write exam-quality multiple-choice questions that test conceptual understanding.\\n- 1 question only, grounded in the given context.\\n- 4 options (A-D), with exactly one correct.\\n- No trivial recall (dates/numbers) unless core to the concept.\\n- Avoid vague or ambiguous wording.\\nReturn strict JSON: {\\\"question\\\": str, \\\"choices\\\": [{\\\"label\\\":\\\"A\\\",\\\"text\\\":...},...], \\\"correct_label\\\":\\\"A\\\"}\"\n",
    "QG_USER_TMPL = \"Idea summary: {idea}\\nUse these supporting snippets (may be partial) to craft 1 conceptual question:\\n{contexts}\\nReturn ONLY the specified JSON.\"\n",
    "\n",
    "import random\n",
    "def shuffle_choices(payload: Dict[str,Any]) -> Dict[str,Any]:\n",
    "    choices = payload[\"choices\"]\n",
    "    # track which is correct BEFORE shuffle\n",
    "    correct = payload[\"correct_label\"]\n",
    "    correct_text = next(c[\"text\"] for c in choices if c[\"label\"]==correct)\n",
    "    # shuffle\n",
    "    labels = [\"A\",\"B\",\"C\",\"D\"]\n",
    "    random.shuffle(choices)\n",
    "    # reassign labels\n",
    "    out_choices = []\n",
    "    new_correct = None\n",
    "    for lab, ch in zip(labels, choices):\n",
    "        out_choices.append({\"label\": lab, \"text\": ch[\"text\"]})\n",
    "        if ch[\"text\"] == correct_text:\n",
    "            new_correct = lab\n",
    "    payload[\"choices\"] = out_choices\n",
    "    payload[\"correct_label\"] = new_correct\n",
    "    return payload\n",
    "\n",
    "def generate_question_for_idea(idea: Dict[str,str], k=3) -> Dict[str,Any]:\n",
    "    ctx_items = retrieve_context_for_idea(idea[\"summary\"], k=k)\n",
    "    ctx_str = \"\\\\n---\\\\n\".join([it[\"chunk\"][:1200] for it in ctx_items])\n",
    "    out = chat(QG_SYSTEM, QG_USER_TMPL.format(idea=idea[\"summary\"], contexts=ctx_str), max_tokens=700)\n",
    "    data = json.loads(out)\n",
    "    data = shuffle_choices(data)\n",
    "    data[\"id\"] = idea[\"id\"]\n",
    "    data[\"idea_summary\"] = idea[\"summary\"]\n",
    "    data[\"source_citations\"] = [f\"{it['meta']['doc']}|{it['meta']['chunk_id']}\" for it in ctx_items]\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c822d51",
   "metadata": {},
   "source": [
    "\n",
    "## Quality Control (Bonus)\n",
    "We score each question on: clarity, groundedness, non-triviality, distractor quality -> average to an overall score.  \n",
    "Threshold (default >= 0.7) filters out weaker items.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdedf5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "QC_SYSTEM = \"You are grading a multiple-choice question with rubric 0.0-1.0.\\nCriteria:\\n- clarity: clear, unambiguous stem\\n- groundedness: answer supported by provided context\\n- non_triviality: requires understanding (not copy-paste recall)\\n- distractor_quality: plausible but clearly incorrect\\nReturn JSON: {\\\"clarity\\\":x,\\\"groundedness\\\":x,\\\"non_triviality\\\":x,\\\"distractor_quality\\\":x,\\\"notes\\\":str}\"\n",
    "QC_USER_TMPL = \"Question:\\n{q}\\nChoices: {choices}\\nCorrect: {correct}\\nContext (evidence):\\n{ctx}\\n\"\n",
    "\n",
    "def score_question(item: Dict[str,Any]) -> Dict[str,Any]:\n",
    "    ctx = \"\\\\n---\\\\n\".join(item[\"source_citations\"])\n",
    "    q = item[\"question\"]\n",
    "    ch = \"; \".join([f\"{c['label']}) {c['text']}\" for c in item[\"choices\"]])\n",
    "    out = chat(QC_SYSTEM, QC_USER_TMPL.format(q=q, choices=ch, correct=item[\"correct_label\"], ctx=ctx), max_tokens=400)\n",
    "    try:\n",
    "        scores = json.loads(out)\n",
    "    except Exception:\n",
    "        scores = {\"clarity\":0.6,\"groundedness\":0.6,\"non_triviality\":0.6,\"distractor_quality\":0.6,\"notes\":\"parse-fallback\"}\n",
    "    import numpy as np\n",
    "    overall = float(np.mean([scores.get(\"clarity\",0), scores.get(\"groundedness\",0),\n",
    "                             scores.get(\"non_triviality\",0), scores.get(\"distractor_quality\",0)]))\n",
    "    item[\"quality\"] = {\"overall\": round(overall,3), **{k: round(float(scores.get(k,0)),3) for k in [\"clarity\",\"groundedness\",\"non_triviality\",\"distractor_quality\"]}, \"notes\": scores.get(\"notes\",\"\")}\n",
    "    return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c26dab5",
   "metadata": {},
   "source": [
    "\n",
    "## Difficulty Tagging (Bonus)\n",
    "We map Bloom levels -> easy/medium/hard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f201bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DIFF_SYSTEM = \"You are a psychometrics expert. Classify the question's Bloom level (Remember, Understand, Apply, Analyze, Evaluate, Create).\\nReturn JSON: {\\\"bloom\\\": \\\"Understand\\\"}\"\n",
    "\n",
    "def add_difficulty(item: Dict[str,Any]) -> Dict[str,Any]:\n",
    "    text = item[\"question\"] + \" Choices: \" + \"; \".join([c[\"text\"] for c in item[\"choices\"]])\n",
    "    out = chat(DIFF_SYSTEM, text, max_tokens=100)\n",
    "    try:\n",
    "        bloom = json.loads(out).get(\"bloom\",\"Understand\")\n",
    "    except Exception:\n",
    "        bloom = \"Understand\"\n",
    "    mapping = {\"Remember\":\"easy\",\"Understand\":\"easy\",\"Apply\":\"medium\",\"Analyze\":\"medium\",\"Evaluate\":\"hard\",\"Create\":\"hard\"}\n",
    "    item[\"difficulty\"] = mapping.get(bloom, \"medium\")\n",
    "    return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f254929d",
   "metadata": {},
   "source": [
    "\n",
    "## Generate Questions\n",
    "Adjust MAX_QUESTIONS as needed. For long docs, the pipeline amortizes costs and scales better than naive prompting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ac1787",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_QUESTIONS = int(os.environ.get(\"MAX_QUESTIONS\", \"20\"))\n",
    "K_CONTEXT = int(os.environ.get(\"K_CONTEXT\", \"3\"))\n",
    "QUALITY_THRESHOLD = float(os.environ.get(\"QUALITY_THRESHOLD\", \"0.70\"))\n",
    "\n",
    "results = []\n",
    "for idea in tqdm(ideas[:MAX_QUESTIONS], desc=\"Generating Qs\"):\n",
    "    item = generate_question_for_idea(idea, k=K_CONTEXT)\n",
    "    item = score_question(item)               # bonus\n",
    "    item = add_difficulty(item)               # bonus\n",
    "    results.append(item)\n",
    "\n",
    "# Filter by quality\n",
    "filtered = [r for r in results if r.get(\"quality\",{}).get(\"overall\",0) >= QUALITY_THRESHOLD]\n",
    "print(f\"Generated: {len(results)}  /  Kept after quality filter (>={QUALITY_THRESHOLD}): {len(filtered)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41a2b15",
   "metadata": {},
   "source": [
    "\n",
    "## Save JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27e0749",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "out = {\n",
    "    \"run_metadata\": {\n",
    "        \"created_at\": datetime.datetime.now().isoformat(),\n",
    "        \"docs\": docs_names,\n",
    "        \"model\": OPENAI_MODEL,\n",
    "        \"embedding_model\": OPENAI_EMBEDDING,\n",
    "        \"k_context\": K_CONTEXT,\n",
    "        \"cost_estimate_usd\": None  # optionally compute using tiktoken counts * $/1K\n",
    "    },\n",
    "    \"questions\": filtered\n",
    "}\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "with open(OUTPUT_DIR / \"questions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(out, f, ensure_ascii=False, indent=2)\n",
    "print(\"Saved ->\", OUTPUT_DIR / \"questions.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059ad5ba",
   "metadata": {},
   "source": [
    "\n",
    "## Appendix: 30-second Architecture\n",
    "```\n",
    "docs/ (PDF, txt)\n",
    "   |\n",
    "   |-- parse & chunk (token-aware windows)\n",
    "   |         |\n",
    "   |         `-- map->combine->reduce: extract conceptual ideas (LLM)\n",
    "   |                      |\n",
    "   |-- embed chunks ------+--> FAISS index\n",
    "   |                      |\n",
    "   `-- per-idea retrieve top-k context\n",
    "                          |\n",
    "                      question generation (LLM)\n",
    "                          |\n",
    "               quality scoring & difficulty (LLM)\n",
    "                          |\n",
    "                   output/questions.json\n",
    "```\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
